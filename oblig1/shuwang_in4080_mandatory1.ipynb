{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 1 (Autumn 2025)\n",
    " \n",
    "Mandatory assignment 1 consists of three parts. In Part 1 (6 points), you will test and improve on a BPE (Byte-Pair-Encoding) tokenizer . In Part 2 (7 points), you will estimate an N-gram language model, based on a training corpus and the tokenizer you worked on in Part 1. Finally, in Part 3 (7 points), you will develop a basic classification model to distinguish between Bokmål and Nynorsk sentences.\n",
    "\n",
    "You should answer all three parts. You are required to get at least 12/20 points to pass. The most important is that you try to answer each question (possibly with some mistakes), to help you gain a better and more concrete understanding of the topics covered during the lectures. There are also bonus questions for those of you who would like to deepen their understanding of the topics covered by this assignment.\n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Friday, September 12 at 23:59__), but include all files in the last delivery.\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory1_\n",
    "- You can work on this assignment either on the IFI machines or on your own computer. \n",
    "\n",
    "*The preferred format for the assignment is a completed version of this Jupyter notebook*, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have done and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n",
    "\n",
    "__Technical tip__: Some of the tasks in this assignment will require you to extend methods in classes that are already partly implemented. To implement those methods directly in a Jupyter notebook, you can use the function `setattr` to attach a method to a given class: \n",
    "\n",
    "```python\n",
    "class A:\n",
    "    pass\n",
    "a = A()\n",
    "\n",
    "def foo(self):\n",
    "    print('hello world!')\n",
    "    \n",
    "setattr(A, 'foo', foo)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure that all required modules are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit_learn in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit_learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit_learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit_learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm numpy scikit_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Tokenisation\n",
    "\n",
    "We will start by building a basic tokenizer relying on white space and punctuation. \n",
    "\n",
    "__Task 1.1__ (2 points): Implement the method `split` below such that it takes a text as input and outputs a list of tokens. The tokenisation should simply be done by splitting on white space, except for punctuation markers and other symbols (`.,:;!?-()\"`), which should correspond to their own token. For instance, the sentence \"Pierre, who works at NR, also teaches at UiO.\" should be split into 12 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"The method should split the text on white space, except for punctuation\n",
    "    markers that should be considered as tokens of their own (even in the \n",
    "    absence of white space before or after their occurrence)\"\"\"\n",
    "\n",
    "    # Implement here your basic tokenisation\n",
    "    \n",
    "    return re.findall(r\"\\w+|[.,:;!?\\-()\\\"]\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Pierre, who works at NR, also teaches at UiO.\"\n",
    "token_a = basic_tokenize(a)\n",
    "len(token_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explanation:\n",
    "Since the regular expression package re is imported, we can use the findall function to extract tokens. The pattern \\w+ is used to capture words. Still, we need to handle punctuation symbols so that they become separate tokens. \n",
    "\n",
    "Obs!: - and \" must be handled with a backslash (\\-), otherwise we will get a “bad character” error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the tokeniser on a small corpus, the [Norwegian Dependency Treebank](https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-10/) (the corpus has been annotated with morphological features, syntactic functions and hierarchical structures, but we'll simply use here the raw text and discard all the annotation layers). We provide you with the data in the files `ndt_train_lm.txt` and `ndt_test_lm.txt`. \n",
    "\n",
    "__Task 1.2__ (1 point): Run the tokenizer you have implemented on `ndt_test_lm.txt`. How many tokens were extracted? And how many types (distinct words) were there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('ndt_train_lm.txt', 'r', encoding='utf-8-sig') as f:\n",
    "  text_train = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258079"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_solution: the output of basic_tokenize function is a list of tokens. The difference between tokens and types(distinct words) is that types do not have repeated words. So we just need to take away the repeated words in a list. We can simply use set() to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30006"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_types = set(test_tokens)\n",
    "len(test_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use Byte-Pair Encoding (BPE) to limit the vocabulary of the tokenizer to 5,000.  An initial implementation of the algorithm is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Iterator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"Tokenizer based on the Byte-Pair Encoding algorithm. \n",
    "    Note: the current implementation is limited to Latin characters (ISO-8859-1)\"\"\"\n",
    "    #def __init__(self, train_corpus_file: str, vocab_size = 500):\n",
    "    def __init__(self, train_corpus_file: str, vocab_size = 5000): \n",
    "        \"\"\"Creates a new BPE tokenizer, with merge pairs found using the given\n",
    "        corpus file. The extraction of merge pairs stops when a vocabulary of \n",
    "        size vocab_size is reached.\"\"\"\n",
    "\n",
    "        # List of string pairs that should be merged when tokenizing\n",
    "        # Example: ('e', 't'), which means that 'et' is a possible subword\n",
    "        # Each string pair is mapped to an unique index number\n",
    "        # (corresponding to their position in the self.vocab list)\n",
    "        self.merge_pairs = {}\n",
    "\n",
    "        # We add as basic vocab all characters of the extended ASCII\n",
    "        self.vocab = [chr(i) for i in range(256)]\n",
    "\n",
    "        ## Shu add###\n",
    "        # identify punct\n",
    "        self.punct = set('.,:;!?-()\"')\n",
    "        #########\n",
    "       \n",
    "\n",
    "        with open(train_corpus_file) as fd:\n",
    "\n",
    "            # We first read the corpus, split on white space, and counts the\n",
    "            # occurrences of each distinct word\n",
    "            #print(\"Counting word occurrences in corpus %s\"%train_corpus_file, end=\"...\", flush=True)\n",
    "            text = fd.read()\n",
    "            vocabulary_counts = {}\n",
    "            for token in text.split():\n",
    "                vocabulary_counts[token] = vocabulary_counts.get(token, 0) + 1\n",
    "            #print(\"Done\")\n",
    "\n",
    "            # We then iteratively extend the list of merge pairs until we\n",
    "            # reach the desired size. Note: to speed up the algorithm, we \n",
    "            # extract n merge pairs at each iteration\n",
    "            #progress_bar = tqdm(total=vocab_size)\n",
    "            while len(self.vocab) < vocab_size:\n",
    "                most_common_pairs = self.get_most_common_pairs(vocabulary_counts)      \n",
    "                for common_pair in most_common_pairs:\n",
    "                    self.merge_pairs[common_pair] = len(self.vocab)\n",
    "                    self.vocab.append(\"\".join(common_pair))\n",
    "                    if len(self.vocab) >= vocab_size:\n",
    "                        break\n",
    "                #progress_bar.update(len(most_common_pairs))\n",
    "                #print(\"Examples of new subwords:\", [\"\".join(pair) for pair in most_common_pairs][:10]) ##\n",
    "            \n",
    "    def get_most_common_pairs(self, vocabulary_counts: Dict[str,int], \n",
    "                              n:int=200) -> List[Tuple[str,str]]:\n",
    "        \"\"\"Given a set of distinct words along with their corresponding number \n",
    "        of occurrences in the corpus, returns the n most frequent pairs of subwords.       \n",
    "        \"\"\"\n",
    "\n",
    "        # We count the frequencies of consecutive subwords in the vocabulary list\n",
    "        pair_freqs = {}\n",
    "        for word, word_count in vocabulary_counts.items():\n",
    "            subwords = self.tokenize_word(word)\n",
    "            \n",
    "            for i in range(len(subwords)-1):\n",
    "                byte_pair = (subwords[i], subwords[i+1])\n",
    "                \n",
    "                ##Shu add: skip punctuation-involved pairs########\n",
    "                if any(char in self.punct for char in byte_pair[0]) or any(char in self.punct for char in byte_pair[1]):\n",
    "                    continue\n",
    "                ################################################    \n",
    "        \n",
    "                pair_freqs[byte_pair] = pair_freqs.get(byte_pair, 0) + word_count\n",
    "\n",
    "        # And return the most frequent ones\n",
    "        most_freq_pairs = sorted(pair_freqs.keys(), key=lambda x: pair_freqs[x])[::-1][:n]\n",
    "        return most_freq_pairs\n",
    "\n",
    "    def __call__(self, input:str, show_progress_bar=True) -> Iterator[str]:\n",
    "        \"\"\"Tokenizes a full text\"\"\"\n",
    "\n",
    "        # We first split into whitespace-separated tokens, and then in subwords\n",
    "        words = input.split()\n",
    "        #for word in tqdm(words) if show_progress_bar else words:\n",
    "        for word in words:\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for subword in subwords:\n",
    "                yield subword\n",
    "                \n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        \"\"\"Splits the word into subwords, according to the merge pairs \n",
    "        currently stored in self.merge_pairs.\"\"\"\n",
    "\n",
    "        # We start with a list of characters\n",
    "        # (+ a final character to denote the end of the word)    \n",
    "        splits = list(word) + [\" \"]\n",
    "\n",
    "        # We continue until there is nothing left to be merged\n",
    "        while len(splits)>=2:\n",
    "\n",
    "            # We extract consecutive subword pairs\n",
    "            pairs = [(splits[i], splits[i+1]) for i in range(len(splits)-1)]\n",
    "\n",
    "            # We find the \"best\" pair of subwords to merge -- that is, the one with the \n",
    "            # lowest position in the list of merge rules\n",
    "            best_pair_to_merge = min(pairs, key=lambda x: self.merge_pairs.get(x, np.inf))\n",
    "            if best_pair_to_merge in self.merge_pairs:\n",
    "\n",
    "                # We then merge the two subwords\n",
    "                for i in range(len(splits)-1):\n",
    "                    if (splits[i], splits[i+1]) == best_pair_to_merge:\n",
    "                        merged_subword = self.vocab[self.merge_pairs[best_pair_to_merge]]\n",
    "                        splits = splits[:i] + [merged_subword] + splits[i+2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.3__ (1 point): Learn the BPE tokenizer on the `ndt_train_lm.txt` corpus, and then apply this tokenizer on `ndt_test_lm.txt`. Print the number of tokens and types (distinct subwords) obtained by this tokenizer on the test data. How do those numbers compare to the ones obtained with the basic tokenizer you had implemented earlier ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the BPETokenizer based on training data.\n",
    "tokenizer = BPETokenizer('ndt_train_lm.txt',vocab_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the trained tokenizer to the test data\n",
    "test_new_tokens = []\n",
    "for token in tokenizer(text_test):\n",
    "    if token != \" \": #this is because in the BPE tokenizer, we add a \" \"(space) to denote the end of the word. We need to get rid of it. \n",
    "        test_new_tokens.append(token)\n",
    "#test_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398709"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test_new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explain:\n",
    "We can see that the token length using the BPETokenizer has increased compared to when using the basic tokenizer. The reason is in basic tokenizer the minimum unit is word. However, the minimum unit in BPETokenizer is subword. For example, the token 'Lam' has turned to 'La','m'. This explains why the number of tokens using BPETokenizer increased compared to the basic tokenizer.\n",
    "\n",
    "The number of types has been greatly reduces when using BPETokenizer. The reason is since word has been splitted to subword, the subword can be combined in different orders to form different word. So the vocabulary of subwords is more compact and flexible. Another reason is, BPETokenizer has limit the vocabulary to a subword up to 5,000. So the BPETokenizer has filtered out the rare tokens compared to basic tokenizer. Thus BPETokenizer produces a much more concise and manageable vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.4__ (2 points): The current BPE implementation is that it treats all characters in the same manner. A rather inconvenient side effect is that letters may be merged together with punctuation markers (like 'ing', ',' --> 'ing,'), if they are not separated by white space. Modify the implementation of the BPE algorithm above to prevent punctuation markers to be merged with letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_solution: \n",
    "In this task, we need to avoid subword merging with punctuation. So first, we define a set of punctuation characters as self.punct.\n",
    "Then before merging two common_pairs, we need to identify if punctuation exist in either common_pair. If yes, we skip merging using continue. otherwise, we proceed the combine work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.5__ (_optional, 2 extra points_): In a [tweet](https://x.com/karpathy/status/1759996551378940395) published last year, the well-known AI researcher Andrej Karpathy stressed that many of the current limitations of Large Language Models are actually a product of the tokenisation step. Explain at least 4 of the problems he mentioned in his tweet (you can of course search online, or watch Karpathy's own video lecture on tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: N-gram language models\n",
    "\n",
    "We will now train simple N-gram language models on the NDT corpus, using the tokenizers we have developed in Part 1.\n",
    "\n",
    "Here is the skeleton of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"Generic class for running operations on language models, using a BPE tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BPETokenizer):\n",
    "        \"\"\"Build an abstract language model using the provided tokenizer\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    " \n",
    "    @abstractmethod\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of context tokens (=previous tokens), returns a dictionary\n",
    "          mapping each possible token to its probability\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_perplexity(self, text: str):\n",
    "        \"\"\"Computes the perplexity of the given text according to the LM\"\"\"\n",
    "\n",
    "        print(\"Tokenising input text:\")\n",
    "        tokens = list(self.tokenizer(text))\n",
    "        \n",
    "        print(\"Computing perplexity:\")\n",
    "        log_probs = 0\n",
    "        #for i in tqdm(range(len(tokens))):\n",
    "        for i in range(len(tokens)):    \n",
    "            context_tokens = [\"<s>\"] + tokens[:i]\n",
    "            predict_distrib = self.predict(context_tokens)\n",
    "\n",
    "            # We add the log-probabilities\n",
    "            log_probs += np.log(predict_distrib[tokens[i]])\n",
    "            \n",
    "        perplexity = np.exp(-log_probs/len(tokens))\n",
    "        return perplexity\n",
    "\n",
    "class NGramLanguageModel(LanguageModel):\n",
    "    \"\"\"Representation of a N-gram-based language model\"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus_file: str, tokenizer:BPETokenizer, ngram_size:int=3,\n",
    "                  alpha_smoothing:float=1):\n",
    "        \"\"\"Initialize the N-gram model with:\n",
    "        - a file path to a training corpus to estimate the N-gram probabilities\n",
    "        - an already learned BPE tokenizer\n",
    "        - an N-gram size\n",
    "        - a smoothing parameter (Laplace smoothing)\"\"\"\n",
    "        \n",
    "        LanguageModel.__init__(self, tokenizer)\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        # We define a simple backoff distribution (here just a uniform distribution)\n",
    "        self.default_distrib = {token:1/len(tokenizer.vocab) for token in tokenizer.vocab}\n",
    "\n",
    "        # Dictionary mapping a context (for instance the two preceding words if ngram_size=3)\n",
    "        # to another dictionary specifying the probability of each possible word in the \n",
    "        # vocabulary. The context should be a tuple of tokens.\n",
    "        self.ngram_probs = {}\n",
    "        with open(training_corpus_file) as fd:   \n",
    "\n",
    "            # based on the training corpus, tokenizer, ngram-size and smoothing parameter,\n",
    "            # fill the self.ngram_probs with the correct N-gram probabilities  \n",
    "            raise NotImplementedError()\n",
    " \n",
    "\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of preceding tokens, returns the probability distribution \n",
    "        over the next possible token.\"\"\"\n",
    "\n",
    "        # We restrict the contextual tokens to (N-1) tokens\n",
    "        context_tokens = tuple(context_tokens[-self.ngram_size+1:])\n",
    "\n",
    "        # If the contextual tokens were indeed observed in the corpus, simply\n",
    "        # returns the precomputed probabilities\n",
    "        if context_tokens in self.ngram_probs:\n",
    "            return self.ngram_probs[context_tokens]\n",
    "        \n",
    "        # Otherwise, we return a uniform distribution over possible tokens\n",
    "        else:\n",
    "            return self.default_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ (6 points): Complete the initialization method `__init__` to estimate the correct N-gram probabilities (with smoothing) based on the corpus. Don't worry about making your implementation super-efficient (although you can if you wish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def __init__(self, training_corpus_file: str, tokenizer:BPETokenizer, ngram_size:int=2, alpha_smoothing=0.1):\n",
    "        \"\"\"Initialize the N-gram model with:\n",
    "        - a file path to a training corpus to estimate the N-gram probabilities\n",
    "        - an already learned BPE tokenizer\n",
    "        - an N-gram size\n",
    "        - a smoothing parameter (Laplace smoothing)\"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        LanguageModel.__init__(self, tokenizer)\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        # We define a simple backoff distribution (here just a uniform distribution)\n",
    "        self.default_distrib = {token:1/len(tokenizer.vocab) for token in tokenizer.vocab}\n",
    "\n",
    "        # Dictionary mapping a context (for instance the two preceding words if ngram_size=3)\n",
    "        # to another dictionary specifying the probability of each possible word in the \n",
    "        # vocabulary. The context should be a tuple of tokens.\n",
    "        self.ngram_probs = {}\n",
    "    \n",
    "        with open(training_corpus_file) as fd:  \n",
    "            \n",
    "            # ADD HERE YOUR CODE TO FILL THE VALUES IN self.ngram_probs\n",
    "\n",
    "            #generate tokens\n",
    "            text = fd.read()\n",
    "            #tokens = list(tokenizer(text))\n",
    "            \n",
    "            tokens = [\"<s>\"] * (self.ngram_size - 1) + list(tokenizer(text))\n",
    "            #print(\"Number of tokens in training data:\", len(tokens))\n",
    "            #print(\"First 20 tokens:\", tokens[:20])\n",
    "\n",
    "\n",
    "            # construct the right data format according to the ngram_size, when ngram_size=3\n",
    "            # it means ngrams is a list with tuples (each tuple size=3)\n",
    "            # I construct it to list type, because it is easier to use Counter in list\n",
    "            # The context is basically ngram drop the last element,which is the target\n",
    "            \n",
    "            ngrams = [tuple(tokens[i:i+self.ngram_size]) for i in range(len(tokens)-self.ngram_size+1)]\n",
    "            #ngrams = [tuple(tokens[i:i+self.ngram_size]) for i in range(100-self.ngram_size+1)]\n",
    "            contexts = [item[:-1] for item in ngrams]\n",
    "\n",
    "            ngram_counts = Counter(ngrams)\n",
    "            context_counts = Counter(contexts)\n",
    "            #print(ngram_counts,'--------------',context_counts)\n",
    "            print(f\"ngram_counts: {len(ngram_counts)}, context_counts: {len(context_counts)}\")\n",
    "\n",
    "\n",
    "            ## Now calculate the probability based on the count result\n",
    "            ## Here we apply Markov assumption, which simplifies the problem by restricting the conditional probabilities\n",
    "            ## to the k previous words. when ngram_size=3, k=2\n",
    "            ## Here we need to consider Laplace smoothing to avoid target appears in testing set not training set\n",
    "            ## We limit the computation by only run a fraction of context_counts (20000 context tokens)\n",
    "\n",
    "\n",
    "            for context_tokens in list(context_counts)[:20000]: #[改] Here we run only a fraction of context_counts\n",
    "                denominator = context_counts[context_tokens] + alpha_smoothing * (len(self.tokenizer.vocab))\n",
    "                distrib = {\n",
    "                            token: (ngram_counts.get((*context_tokens, token), 0) + alpha_smoothing) / denominator\n",
    "                            for token in self.tokenizer.vocab\n",
    "                            #for token in self.tokenizer.vocab[:100]\n",
    "                        }\n",
    "                self.ngram_probs[context_tokens] = distrib\n",
    "                \n",
    "            ctx = ('<s>','No',)\n",
    "            sorted_probs = sorted(self.ngram_probs[ctx].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Top 5 predictions:\", sorted_probs[:5])                       \n",
    "\n",
    "setattr(NGramLanguageModel, '__init__', __init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explaination:\n",
    "In this task, we first tokenize the ndt_train_lm and generate the tokens for the training data, using the BPETokenizer we implemented in task 1. \n",
    "Then we need to construct the data to the right format, we did a little test sample below to illustrate the format we want to have in the ngrams, when ngram_size=3, we want to have 3 tokens in 1 tuple, in this tuple, the last element is our target, and first two are our context. \n",
    "When we have ngrams, it is easy to get contexts, simple drop the last elements of ngram then we have context.\n",
    "Now we have the right format data ngrams and contexts. We can use Counter function to count how many times each tuple occurs. \n",
    "1. ngram_counts: to count the number of occurance of (context1, context2, target)\n",
    "2. context_counts: to count the occurrance of the (context1, context2)\n",
    "When we have all the count numbers, we can calculate probabilities using Markov assumption.\n",
    "We use for loop of tokens, to add token to the context and check if it appears in ngrams_counts, if not apear, then value = 0\n",
    "When target doesn't exist in training set,then the predict probability will be 0, and in the perplexity we use log, then it is meaningless.So we need to introduce Laplace smoothing. By doing this, even if the target never appear in the training set, it will still have a very small probability. In LLM next word prediction, we only choose the token with the hightest probability.\n",
    "After probabilitiy is calculated, put the context_tokens and probability into ngram_probs.\n",
    "\n",
    "Shu_reflection:\n",
    "When constructing the ngram_probs dictionary, we rely on nested loops. This causes the data size to grow extremely large — roughly proportional to context size × vocabulary size. As a result, building the full distribution can be very time-consuming. During testing, we reduced both the vocabulary size and the number of contexts, which allowed us to quickly verify the correctness of our implementation. However, this simplification introduces certain limitations, since it does not reflect the performance of the full model. We will revisit this issue in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3), (2, 3, 4), (3, 4, 1), (4, 1, 2), (1, 2, 4), (2, 4, 5), (4, 5, 6)] Counter({(1, 2, 3): 1, (2, 3, 4): 1, (3, 4, 1): 1, (4, 1, 2): 1, (1, 2, 4): 1, (2, 4, 5): 1, (4, 5, 6): 1}) [(1, 2), (2, 3), (3, 4), (4, 1), (1, 2), (2, 4), (4, 5)] Counter({(1, 2): 2, (2, 3): 1, (3, 4): 1, (4, 1): 1, (2, 4): 1, (4, 5): 1})\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from collections import defaultdict, Counter\n",
    "a = [1,2,3,4,1,2,4,5,6]\n",
    "b = [tuple(a[i:i+3]) for i in range(len(a)-3+1)]\n",
    "c= [item[:-1] for item in b]\n",
    "print(b,Counter(b),c,Counter(c))\n",
    "#ngrams = [tuple(tokens[i:i+self.ngram_size]) for i in range(len(tokens)-self.ngram_size+1)]\n",
    "#contexts = [ng[:-1] for ng in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__ (1 point): Train your language model in `ndt_train_lm.txt`, and compute its perplexity on the test data in `ndt_test_lm.txt`. The perplexity can be computed by calling the method `get_perplexity`. <br>\n",
    "(_Note_: if the training takes too much time, feel free to stop the process after looking at a fraction of the corpus, at least while you are testing/developing your training setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e04468b95840e5ad1d58ea153a6e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_counts: 624814, context_counts: 262225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc243b34a3547edae6d295a2289e668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions: [('kre ', 0.00219560878243513), ('\\x00', 0.00019960079840319363), ('\\x01', 0.00019960079840319363), ('\\x02', 0.00019960079840319363), ('\\x03', 0.00019960079840319363)]\n"
     ]
    }
   ],
   "source": [
    "lm = NGramLanguageModel(\"ndt_train_lm.txt\",tokenizer, ngram_size=3, alpha_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ndt_test_lm.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    print(\"text_test size: {len(f.read())}\")\n",
    "    text_test=f.read()\n",
    "    #text_test=f.read(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising input text:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46220d75bd574f8aa8640d2bc2b305e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23f8ad8e6744c2ba1ae542e2a74d352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/428326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488.310784102801\n"
     ]
    }
   ],
   "source": [
    "test_perplexity = lm.get_perplexity(text_test)\n",
    "print(test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_reflection:\n",
    "As mentioned in the previous task, I limited the size of context_counts to 20,000 (compared to the original 262,225). This reduction achieved computational efficiency but resulted in insufficient training and consequently a higher perplexity. When I increased the size of context_counts, there is a clear improvement in perplexity. Similarly, for efficiency in testing, I restricted the test data size to 50000 tokens. This makes the computation of perplexity much quickly. When I later increased the test data to its full size, the perplexity only improved slightly. One key learning is that in real scenarios, it may be sufficient to evaluate perplexity on a reasonable proportion of the test data. This approach is significantly time-saving and energy-efficient, while still providing reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__ (_optional_, 4 bonus points): Improve the language model you have just developed. You can choose to focus on improving your model through a backoff mechanism, interpolation, or both. Once you are done, compute the perplexity again on the test corpus to ensure the language model has indeed improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text classification\n",
    "\n",
    "We will finally use the texts from the Norwegian Dependency Treebank for a classification task -- more precisely to determine whether a sentence is likely to be written in Bokmål or Nynorsk. To this end, we will use a simple bag-of-word setup (or more precisely a bag-of-_subwords_, since we will rely on the subwords extracted using BPE) along with a logistic regression model. As there is only two, mutually exclusive classes, you can view the task as a binary classification problem. \n",
    "\n",
    "The training data is found in `ndt_train_class.txt` and simply consists of a list of sentences, each sentence being mapped to a language form (Bokmål: `nob` or Nynorsk: `nno`). The language form is written at the end of each line, separated by a `\\t`. Note the training examples are currently _not_ shuffled.\n",
    "\n",
    "To train and apply your classifier, the easiest is to use the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.1__ (2 points): Create a `N x V` matrix in which each line corresponds to a training example (out of `N` training instances) and each row corresponds to an individual feature, in this case the presence/absence of a particular subword in the sentence. In other words, there should be a total of `V` features, where `V` is the total size of the vocabulary for our BPE tokenizer. Also create a vector of length `N` with a value of `1` if the sentence was marked as Nynorsk, and 0 if is was marked as Bokmål. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeg husker også Jens Stoltenbergs uttalelse ti...</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeg har fulgt med i mange år, og ikke sett noe...</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folk i Albania er stolte over NATO-medlemskapet.</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spesielt i Lindesnes og Lyngdal er det svært m...</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Om ikkje det er ein mannleg ønskedraum, så ve...</td>\n",
       "      <td>nno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29901</th>\n",
       "      <td>Libero er ikke alene om å pushe ensidig baby- ...</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29902</th>\n",
       "      <td>Legg appelsinsteinane i eit klede, og knyt att...</td>\n",
       "      <td>nno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29903</th>\n",
       "      <td>Dette er boka om Jerusalem som eg alltid har h...</td>\n",
       "      <td>nno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29904</th>\n",
       "      <td>Martin Bengtsson spiller denne siste sangen fo...</td>\n",
       "      <td>nob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29905</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29906 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               instances label\n",
       "0      Jeg husker også Jens Stoltenbergs uttalelse ti...   nob\n",
       "1      Jeg har fulgt med i mange år, og ikke sett noe...   nob\n",
       "2       Folk i Albania er stolte over NATO-medlemskapet.   nob\n",
       "3      Spesielt i Lindesnes og Lyngdal er det svært m...   nob\n",
       "4      \"Om ikkje det er ein mannleg ønskedraum, så ve...   nno\n",
       "...                                                  ...   ...\n",
       "29901  Libero er ikke alene om å pushe ensidig baby- ...   nob\n",
       "29902  Legg appelsinsteinane i eit klede, og knyt att...   nno\n",
       "29903  Dette er boka om Jerusalem som eg alltid har h...   nno\n",
       "29904  Martin Bengtsson spiller denne siste sangen fo...   nob\n",
       "29905                                                     None\n",
       "\n",
       "[29906 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open('ndt_train_class.txt', 'r', encoding='utf-8-sig') as f:\n",
    "  text_train = f.read()\n",
    "b = [sent.split('\\t') for sent in text_train.split('\\n')]\n",
    "df =  pd.DataFrame(data=b,columns=['instances','label'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeg husker også Jens Stoltenbergs uttalelse ti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeg har fulgt med i mange år, og ikke sett noe...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folk i Albania er stolte over NATO-medlemskapet.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spesielt i Lindesnes og Lyngdal er det svært m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Om ikkje det er ein mannleg ønskedraum, så ve...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29901</th>\n",
       "      <td>Libero er ikke alene om å pushe ensidig baby- ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29902</th>\n",
       "      <td>Legg appelsinsteinane i eit klede, og knyt att...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29903</th>\n",
       "      <td>Dette er boka om Jerusalem som eg alltid har h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29904</th>\n",
       "      <td>Martin Bengtsson spiller denne siste sangen fo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29905</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29906 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               instances  label\n",
       "0      Jeg husker også Jens Stoltenbergs uttalelse ti...    0.0\n",
       "1      Jeg har fulgt med i mange år, og ikke sett noe...    0.0\n",
       "2       Folk i Albania er stolte over NATO-medlemskapet.    0.0\n",
       "3      Spesielt i Lindesnes og Lyngdal er det svært m...    0.0\n",
       "4      \"Om ikkje det er ein mannleg ønskedraum, så ve...    1.0\n",
       "...                                                  ...    ...\n",
       "29901  Libero er ikke alene om å pushe ensidig baby- ...    0.0\n",
       "29902  Legg appelsinsteinane i eit klede, og knyt att...    1.0\n",
       "29903  Dette er boka om Jerusalem som eg alltid har h...    1.0\n",
       "29904  Martin Bengtsson spiller denne siste sangen fo...    0.0\n",
       "29905                                                       NaN\n",
       "\n",
       "[29906 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\"nob\": 0, \"nno\": 1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeg husker også Jens Stoltenbergs uttalelse ti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeg har fulgt med i mange år, og ikke sett noe...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folk i Albania er stolte over NATO-medlemskapet.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spesielt i Lindesnes og Lyngdal er det svært m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Om ikkje det er ein mannleg ønskedraum, så ve...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29900</th>\n",
       "      <td>Broren John har selv vært rammet av kontroverser.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29901</th>\n",
       "      <td>Libero er ikke alene om å pushe ensidig baby- ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29902</th>\n",
       "      <td>Legg appelsinsteinane i eit klede, og knyt att...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29903</th>\n",
       "      <td>Dette er boka om Jerusalem som eg alltid har h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29904</th>\n",
       "      <td>Martin Bengtsson spiller denne siste sangen fo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29905 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               instances  label\n",
       "0      Jeg husker også Jens Stoltenbergs uttalelse ti...    0.0\n",
       "1      Jeg har fulgt med i mange år, og ikke sett noe...    0.0\n",
       "2       Folk i Albania er stolte over NATO-medlemskapet.    0.0\n",
       "3      Spesielt i Lindesnes og Lyngdal er det svært m...    0.0\n",
       "4      \"Om ikkje det er ein mannleg ønskedraum, så ve...    1.0\n",
       "...                                                  ...    ...\n",
       "29900  Broren John har selv vært rammet av kontroverser.    0.0\n",
       "29901  Libero er ikke alene om å pushe ensidig baby- ...    0.0\n",
       "29902  Legg appelsinsteinane i eit klede, og knyt att...    1.0\n",
       "29903  Dette er boka om Jerusalem som eg alltid har h...    1.0\n",
       "29904  Martin Bengtsson spiller denne siste sangen fo...    0.0\n",
       "\n",
       "[29905 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(df[\"instances\"].tolist())\n",
    "with open(\"ndt_train_class_text.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explaination:\n",
    "When tokenize our training data, I limit the vocab_size to 800 to reduce the computation of bag-of-words feature vector N(29905)*vocab_size. We expect the discriminator would be 'ikke' and 'ikkje'. When the vocab_size is downsized to 800, we can see these discriminator still exist in the subwords. \n",
    "Shu_reflection:\n",
    "The computation is huge and I have to deactivate the tqdm and the progress_bar to avoid Error:IOPub message rate exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the BPETokenizer based on training data.\n",
    "tokenizer_t3 = BPETokenizer('ndt_train_class_text.txt',vocab_size = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explaination:\n",
    "In Scikit-Learn, we can use CountVectorizer or TfIdVectorizer to form the Bag-of-words feature vectors. We chose TfIdVectorizer it can handle discriminator much better. It down-weight the very frequent subword and up-weight the less frequent word but more discriminative subwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(analyzer=tokenizer_t3, lowercase=False) \n",
    "vectorizer = TfidfVectorizer(analyzer=tokenizer_t3,lowercase=False,\n",
    "                             vocabulary={tok: i for i, tok in enumerate(tokenizer_t3.vocab)})\n",
    "X_train = vectorizer.fit_transform(df[\"instances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.2__ (2 points): Use the data matrix you have just filled to train a logistic regression model (see the documentation on [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details). We recommend to use the `liblinear` solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;liblinear&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.3__ (1 point): Now apply the learned logistic regression model to the test set in `ndt_test_class.txt`, and evaluate its performance in terms of accuracy, recall and precision (you can use the functionalities in `sklearn.metrics` to compute those easily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data, and preprocess them, splitting instances and label, map label to binary, drop null data\n",
    "with open('ndt_test_class.txt', 'r', encoding='utf-8-sig') as f:\n",
    "  text_test = f.read()\n",
    "c = [sent.split('\\t') for sent in text_test.split('\\n')]\n",
    "df_test =  pd.DataFrame(data=c,columns=['instances','label'])\n",
    "df_test[\"label\"] = df_test[\"label\"].map({\"nob\": 0, \"nno\": 1})\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trained vectorizer on instances to generate the bag-of-words feature vector:\n",
    "X_test = vectorizer.transform(df_test[\"instances\"])\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8983797796500324, precision: 0.916300848697688, recall: 0.862772113529898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "y_test = df_test['label']\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}, precision: {precision}, recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.4__ (2 points): Inspect the weights learned by your logistic regression model (in `coef_`) and find the 5 subwords that contribute _the most_ to the classification of the sentence in Nynorsk. Also find the 5 subwords that contribute the most to the classification of the sentence in Bokmål. Do those weights make sense, according to what you know about Bokmål and Nynorsk ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 666)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check weights learned by logistic regression model.\n",
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_t3.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explaination:\n",
    "We can see that the weight shape contains only 666 columns, but our tokenizer vocabulary we set to 800. The reason is when we used TfIdVectorizer, if the subword in tokenizer_t3.vocab doesn't show up in the instance, the TfIdVectorizer will drop those never appeared. Thus, the number of columns became smaller than the vocab_size. We changed the code and consolidate the columns size with the vocab_size using the index of the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 800)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.01179525e+00, -4.96835318e-01,  1.88470083e-01, -2.82319580e-01,\n",
       "        0.00000000e+00, -3.71782531e-01, -1.09402337e-01,  4.97677994e-01,\n",
       "       -2.52731970e-02, -1.46653208e-01,  5.21784110e-02, -3.39885377e-01,\n",
       "        1.05294600e-01, -2.71235007e-01, -1.69452536e+00,  5.36728867e-01,\n",
       "        2.02591967e-01,  1.28130954e+00, -4.20858743e-01,  4.59129976e-01,\n",
       "        2.26522583e-01,  3.09170456e-01, -2.73396418e-01,  3.55279648e-02,\n",
       "        2.79727062e-01,  8.30839327e-01,  2.95139557e-01,  7.53514399e-02,\n",
       "        0.00000000e+00, -8.67099292e-02,  0.00000000e+00, -5.84082798e-01,\n",
       "       -6.26037576e-01,  7.06940194e-01, -9.55836780e-01, -1.84498261e+00,\n",
       "       -9.94225572e-01,  1.55231762e+00, -9.26390104e-02, -9.98247800e-01,\n",
       "       -1.05463750e+00,  1.98936124e-01, -2.09740463e-01,  2.64192722e-01,\n",
       "       -4.03392402e-01, -1.12368031e-01,  9.97602271e-02,  7.41732566e-02,\n",
       "       -6.01235768e-01,  4.70660555e-01, -5.90927442e-01, -1.06628683e-01,\n",
       "       -4.33595053e-01,  6.75391751e-02,  1.45170355e-01,  1.07308910e-01,\n",
       "       -8.47983935e-01, -6.64104111e-01, -6.17108772e-01,  1.84508191e+00,\n",
       "        0.00000000e+00,  1.69505705e+00,  0.00000000e+00, -3.92842460e-03,\n",
       "        2.69629163e-01,  7.20441176e-01, -9.07787938e-01, -6.44527795e-01,\n",
       "        6.51381748e-02,  1.28492408e+00, -9.60668884e-02,  1.72431265e-01,\n",
       "       -1.62836598e+00,  5.71402571e-01,  1.57371598e+00,  5.31205648e-01,\n",
       "        5.75224880e-01, -1.58951946e-01, -1.19049085e+00,  1.48152102e-01,\n",
       "       -1.31646233e+00, -7.50891708e-03, -7.06196294e-01, -5.48295579e-02,\n",
       "        1.40788505e-01,  1.73748710e+00, -3.06183543e-01, -7.92145683e-01,\n",
       "       -1.00128407e+00, -5.03710547e-01, -5.43022553e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.62397102e+00,\n",
       "        0.00000000e+00, -1.49257340e+00,  0.00000000e+00, -3.05562246e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -3.35363409e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.33581398e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -7.24077158e-01,\n",
       "        0.00000000e+00,  4.75199294e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -3.20700922e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -6.66751242e-01, -1.90860896e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.28593343e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.35078809e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -3.89012221e-01,  0.00000000e+00,\n",
       "       -1.30292889e+00,  0.00000000e+00, -2.95317733e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -6.03534627e-01, -3.41037397e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "       -7.68571138e-02,  6.10170472e-01, -9.07749447e-01, -1.93745051e-01,\n",
       "       -2.39765216e-01, -3.74702345e-01,  1.84206577e+00,  1.43865775e-01,\n",
       "        0.00000000e+00, -1.52424662e-01,  0.00000000e+00,  1.32621419e-01,\n",
       "       -8.16422193e-02, -2.73958130e-02,  3.32340101e+00, -1.79690649e-01,\n",
       "        1.01391126e-01,  0.00000000e+00,  1.56138645e-01,  0.00000000e+00,\n",
       "        2.44090473e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.07792422e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.69769249e+00,  2.99834685e+00, -2.54355803e+00, -1.36201488e+00,\n",
       "       -1.24922816e+00, -2.55868629e+00, -4.05008536e-01,  1.30865841e-01,\n",
       "        1.16252338e+00,  1.43299450e-01,  1.59316486e+00,  3.02804812e+00,\n",
       "        6.16226958e-01,  5.17443220e-01,  8.68184143e-01,  1.80395821e+00,\n",
       "        3.47328320e-01,  1.56370057e+00,  8.65548897e-01, -5.07624336e-01,\n",
       "       -5.33761538e-01,  8.89566566e-01,  5.83989505e-01,  6.58764771e-01,\n",
       "       -4.30243305e-02,  9.22351112e-01,  1.00183728e+00,  2.01626459e+00,\n",
       "       -4.58220367e-01, -1.51278665e+00, -5.65949270e-01, -4.59516315e-01,\n",
       "        2.05588638e+00, -5.67384529e-01,  6.25390014e-01,  5.42701909e+00,\n",
       "        2.36574605e-01,  2.55984849e-01,  5.91981357e-03,  4.24875816e-01,\n",
       "        1.68778489e+00,  1.97789998e+00,  8.04259247e-01,  5.09002579e-01,\n",
       "        7.64469174e-02, -2.68207014e+00, -4.38476768e-01, -5.40876124e-02,\n",
       "        7.74741147e-01,  1.43575037e+00,  9.00741896e-01,  7.12879971e-01,\n",
       "       -2.82542182e-01, -9.54837873e-01,  1.35882771e+00,  2.71166657e+00,\n",
       "        7.47990856e-01,  1.32275333e+00,  1.84929094e+00, -5.56191400e-01,\n",
       "        5.36576902e-01,  1.83294470e-01,  2.67237898e-01, -2.83849038e+00,\n",
       "        1.98328374e+00,  8.64152701e-01, -2.67777431e-01, -2.09254197e-01,\n",
       "       -1.23056749e+00,  1.05107239e-01,  1.04709618e+00,  5.10771087e-01,\n",
       "        4.50632264e-01,  1.19111397e+00, -1.41911377e-02, -5.96072831e-01,\n",
       "        3.31334254e+00,  1.88604879e+00,  2.22404480e+00, -2.71673064e-01,\n",
       "        3.01856215e-01, -3.60851544e-01,  8.25268620e-01,  8.50261984e-01,\n",
       "       -2.66269781e+00,  5.40197564e-01,  2.10735303e+00,  5.69320653e-01,\n",
       "       -2.44933413e-01, -1.19567231e+00,  2.40701357e+00, -6.76282579e-01,\n",
       "        1.23833241e+00, -2.31844949e+00, -2.77420813e-01,  9.63003828e-01,\n",
       "       -1.48682562e+00,  1.28967808e+00, -2.60269334e-01, -1.14064162e-01,\n",
       "       -1.35155041e-01,  3.20555604e+00,  6.87627227e-01, -2.48230405e-01,\n",
       "        1.86939850e-01,  3.31354370e+00,  2.60237348e-01,  1.41571678e+00,\n",
       "        6.01862208e-01, -5.79707274e-01, -6.07227145e-01,  6.09224113e-02,\n",
       "        2.68326939e-01,  1.45894130e-01,  2.91645311e-01, -1.14649275e-01,\n",
       "       -1.37970934e+00,  1.63808108e+00,  9.64090740e-01,  1.19782767e+00,\n",
       "        6.82160025e-01, -1.66629422e-01, -1.33769917e+00,  6.15217458e-02,\n",
       "       -1.05200476e+00, -9.95788835e-01, -1.03592318e+00,  1.16630745e+00,\n",
       "       -1.52964870e+00,  4.58695415e-02,  4.74467790e-01,  6.50505198e-01,\n",
       "       -1.22799839e-01,  7.01844939e-01,  5.69244235e-01,  4.85784721e-01,\n",
       "        2.94012698e-01,  1.32842630e-01,  1.34611280e+00,  8.90144449e-01,\n",
       "        3.81621265e-01,  1.14458629e+00,  4.08202077e-01,  6.46403640e-02,\n",
       "        3.63986672e-01, -1.08279822e+00, -1.66131710e-01,  1.70646990e-01,\n",
       "        2.21780507e+00,  7.83030172e-01,  8.43542585e-01, -3.54774894e-01,\n",
       "        5.96714355e-01,  5.97520375e-01, -3.97003670e-01,  1.56055323e+00,\n",
       "        8.93976522e-01, -8.96784112e-01, -1.80577186e-02, -3.82424820e-01,\n",
       "        6.26965521e-02,  5.11415807e-01, -4.02609860e-01,  1.98856215e+00,\n",
       "        1.03267675e+00, -2.01588867e-01,  1.00621836e+00, -1.27342815e-01,\n",
       "        2.52223701e+00,  1.80458991e-01,  2.44164031e+00,  3.27180032e+00,\n",
       "       -1.76554264e+00,  2.53081034e-02,  7.49365181e-01,  4.23690945e-01,\n",
       "       -1.77164996e-01, -7.34456849e-01, -3.52235980e-01, -7.16176893e-01,\n",
       "        3.80493475e-01, -7.19301272e-01,  4.49885767e-01, -1.19372039e+00,\n",
       "       -9.61199380e-01, -5.18443690e-01, -2.21420293e-01, -2.70546117e-01,\n",
       "       -9.40048940e-01, -2.23950891e+00,  1.17515248e+00,  5.27208668e-01,\n",
       "       -1.00897029e+00,  5.82052993e-01,  1.66154882e+00,  9.27437170e-02,\n",
       "        1.34248460e-01, -1.44039531e+00,  1.02611305e+00,  8.83821972e-01,\n",
       "       -4.28755901e+00,  3.80538324e-01,  1.08956654e+00,  5.03896888e-01,\n",
       "        3.39755485e-01, -1.33554202e+00,  1.49862127e+00,  2.28578615e-01,\n",
       "        4.95879101e-01,  7.63482254e-01,  2.08349787e-01, -1.22750075e+00,\n",
       "       -3.95817482e+00, -3.60980073e+00,  6.68439003e-01,  1.09295766e+00,\n",
       "        1.32196026e-01, -1.73496669e+00,  2.59142491e+00, -6.39574123e-01,\n",
       "       -4.11277048e+00, -1.04036045e+00, -5.71130050e-01, -3.15866204e-01,\n",
       "        3.07734778e-01,  7.51607780e-01,  5.56266472e-01, -5.24533489e+00,\n",
       "        3.37604073e-01, -1.33192192e-01, -1.45250400e-01, -3.30936386e-01,\n",
       "       -1.04063986e-01, -3.98542749e-01, -2.66522440e-01,  1.09060671e+01,\n",
       "       -2.99317621e-01, -4.76746236e+00, -3.62511705e+00, -1.38136570e+00,\n",
       "        8.25804201e-01, -1.34544196e+00, -9.92286988e-01, -4.83216870e+00,\n",
       "        1.14185567e-01,  4.91948029e+00,  4.82037504e-01,  9.95857997e+00,\n",
       "        2.22299487e-01, -7.17367002e-01,  6.08659773e-01, -5.75526288e-01,\n",
       "        9.27111837e-01, -5.80179724e-01,  3.21831044e-01, -8.60709637e-01,\n",
       "        1.29857645e-02,  1.12407459e+00,  7.34653616e-02, -5.44443310e-01,\n",
       "        7.82109129e-01, -3.21893664e-01,  3.57240465e+00,  3.51145758e-01,\n",
       "        4.21006923e-01, -4.58557241e-01, -4.08260484e-01,  1.76474568e+00,\n",
       "        6.19378100e+00, -4.53846342e-01, -2.33520315e+00, -1.33597782e+00,\n",
       "        8.94076826e+00,  1.45138771e-01,  2.56895599e-01, -3.86612350e-01,\n",
       "       -9.37959981e-01, -3.73627171e-03,  9.64863269e+00,  3.79840959e-01,\n",
       "       -2.02786687e+00, -1.27275330e+00,  9.30756303e-01, -3.74540205e+00,\n",
       "        8.55413164e-02, -2.09488642e-01,  9.63759837e-01,  6.76858538e-01,\n",
       "       -2.91696541e+00, -9.23842332e-01, -1.06968708e-01,  5.36170574e-01,\n",
       "        1.41793670e+00,  6.52910601e+00,  6.36429600e-01, -5.35359993e+00,\n",
       "       -6.86504478e-02,  2.00806887e-01, -3.07854385e+00,  1.56427780e+00,\n",
       "        2.63244136e-01,  1.46048175e+00, -1.96576971e-01, -2.85320306e+00,\n",
       "        8.56355394e+00, -4.10723693e-02,  2.69948000e+00,  4.76502752e-01,\n",
       "       -6.16052926e-01,  1.28646391e-01,  5.38941519e-01,  3.19508993e-01,\n",
       "        1.08522995e+00, -9.44515924e-02, -2.14137065e+00, -1.73781189e+00,\n",
       "        4.37156416e-01,  4.86650492e-01,  3.90367249e-01,  8.35343426e-02,\n",
       "        1.55697603e+00, -1.94937211e+00,  2.44260693e-01,  5.48650982e-01,\n",
       "       -2.27815658e+00, -2.97156582e+00,  1.42658981e+00, -6.56754906e+00,\n",
       "        4.28620435e-01,  9.65350662e-01,  8.75551096e+00,  2.23751759e-01,\n",
       "       -2.29785498e-01, -1.89381910e+00, -4.85590012e+00,  1.38376607e+00,\n",
       "       -7.18242634e+00, -1.58798886e-01,  8.10842388e-01,  4.85132587e-01,\n",
       "        6.89826776e-01,  1.95014115e+00,  3.19002497e-01,  9.28560115e-01,\n",
       "        5.16140980e-02,  6.25992183e-02,  1.43384508e+00,  4.76135590e-01,\n",
       "        1.24360920e-01,  3.57165693e+00,  9.82024809e-01,  1.16318331e+00,\n",
       "        2.70014711e+00,  6.81370571e+00,  9.02030325e-01, -4.81559187e-01,\n",
       "       -7.38332800e-02,  9.76473279e-02, -1.68653873e+00, -1.84486636e+00,\n",
       "       -7.12983713e+00,  6.86149223e-02,  9.03379682e-01,  1.12433222e+00,\n",
       "        1.30270540e+00,  1.95741153e-02, -2.71484086e-01,  1.23349885e+00,\n",
       "       -8.88189545e-01, -2.36062434e+00, -6.23250044e-01, -9.83368666e-01,\n",
       "        4.35741963e-01, -2.10119663e+00,  6.53534160e-01,  8.92603123e-01,\n",
       "       -1.79109922e+00, -6.85772412e-01, -6.90720500e-01, -7.14616923e-02,\n",
       "       -3.52059233e+00,  8.91551247e-01, -7.26299619e-01,  9.29414028e-01,\n",
       "        4.61371161e-01,  2.78839998e-01,  1.17061066e+00,  2.28053342e-01,\n",
       "        1.29585374e+00, -5.52633921e+00,  1.12132268e+00,  1.52017350e+00,\n",
       "        0.00000000e+00, -1.19710913e+00,  1.30907565e-01, -3.67256881e-01,\n",
       "       -3.58458912e-01,  3.35467163e+00,  6.10751205e-02, -2.21554684e+00,\n",
       "       -3.69671669e-01,  1.19085852e-01, -9.21224530e+00,  1.11604693e+01,\n",
       "       -4.09523298e-01, -3.48626617e-01,  5.17360265e-01,  8.37285818e-01,\n",
       "        6.97187661e-01,  1.55037851e-02,  2.94800828e-01, -3.56110515e-01,\n",
       "       -3.36088077e+00,  7.35267514e+00,  5.12635033e-02,  3.87171854e-02,\n",
       "       -2.28127222e-01, -1.89489595e+00,  3.97437348e+00,  2.83403912e-01,\n",
       "       -8.68708579e-02,  1.37017942e+00,  3.33788267e+00, -5.49401663e+00,\n",
       "       -2.32845677e+00,  8.15814445e-01, -9.06073064e-01,  2.97708865e-01,\n",
       "        3.78333145e-01, -1.01424447e+00,  7.57818680e-01, -1.30871679e+00,\n",
       "        1.93533483e-01, -1.13300908e-01, -3.44421177e-02, -1.73462514e-01,\n",
       "        5.23610723e-01, -7.95692081e-01, -8.54689346e-01,  3.20520240e-01,\n",
       "        3.04363109e+00, -3.31713393e+00,  8.20288259e-01, -1.06318736e+00,\n",
       "        7.31121326e-01, -1.80071090e+00,  6.28029865e+00,  1.09944100e-01,\n",
       "       -5.65601812e-02, -1.98813268e+00,  9.82513051e-01, -3.14772448e+00,\n",
       "       -1.29979773e+00,  8.25671873e-01,  1.22381547e-01,  5.06934697e-01,\n",
       "        1.01407106e+00,  1.16333901e+00, -6.53808576e+00, -4.31263692e-01,\n",
       "       -5.80683104e+00, -3.81654594e+00, -1.07748996e+00, -6.21200072e+00,\n",
       "       -6.22633784e-01,  1.54475265e+00,  1.37551174e+00,  4.83475392e-01,\n",
       "        8.88253770e-01, -2.16199829e+00, -4.41010661e-02,  8.73122220e-01,\n",
       "        7.59770358e-01, -9.97039645e-01,  7.32969534e-01,  3.57503996e-01,\n",
       "       -6.66736030e-01, -4.59574971e-02, -1.81593914e+00,  1.56836245e+00,\n",
       "        1.18167656e+00,  1.07732026e+00, -5.67033337e-01,  9.83599076e-01,\n",
       "       -5.95057291e-01,  3.62155530e-01,  5.14241355e-02, -1.53301474e+00,\n",
       "       -3.90292662e-01, -2.27089312e-01, -5.97897210e-01,  6.76463135e-01,\n",
       "        9.83504578e-01,  4.70294784e+00, -7.04845993e-01,  1.31533010e-01,\n",
       "       -3.70673962e-03,  7.32086469e-01, -8.90202001e-02, -4.94378212e-01,\n",
       "        4.21485072e+00,  3.69085680e+00,  1.37373807e+00,  1.23429267e+00,\n",
       "        1.52977108e+00,  1.59788871e+00, -1.29937726e-01, -8.46845159e-01,\n",
       "       -6.50024440e-02, -4.94083745e-03,  2.15704119e-01, -1.13567912e+00,\n",
       "        6.10359276e+00, -1.42970497e-01,  4.82636305e-01,  7.35505617e-01,\n",
       "        1.12267661e+00,  2.24821189e+00, -1.42388225e+00,  5.42713931e-01,\n",
       "        1.34937662e+00,  3.26285717e+00,  2.05088909e+00, -1.93475762e+00,\n",
       "        0.00000000e+00, -4.02971683e-01,  6.24898548e+00, -1.13638131e-01,\n",
       "       -6.88582151e-01,  6.24153210e-02, -6.14217013e-02,  3.04813062e+00,\n",
       "       -3.12157660e+00,  8.19185711e-01, -2.71871673e-01,  1.81431813e+00,\n",
       "       -1.06086643e+00,  1.81452273e+00, -9.10525821e-02,  2.32053350e-01,\n",
       "        8.50356678e-01, -1.12490393e+00,  6.91924661e-01,  5.10443958e-01])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_solution:\n",
    "Now we need to find the 5 subwords that contribute the most to the classification of the sentence in Bokmål.\n",
    "Because we map {\"nob\":0, \"nno\":1}, then if the weight is positive, it means it contribute to nynosk, likewise, when the weight is negative, it contribute to bokmål. So now the probelm became to find the top 5 smallest negative weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([658, 592, 616, 583, 714])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef=clf.coef_[0]\n",
    "top_bokmal_index = np.argsort(coef)[:5]\n",
    "top_bokmal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.212245299105692\n",
      "-7.182426336838028\n",
      "-7.129837130541972\n",
      "-6.567549063084499\n",
      "-6.538085758212796\n"
     ]
    }
   ],
   "source": [
    "for i in top_bokmal_index:\n",
    "    print(coef[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ikke ', 'lig ', 'fra ', 'jeg ', 'sier ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer_t3.vocab[i] for i in top_bokmal_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shu_explaination:\n",
    "The bokmål and nynorsk mapping realtion can reveal that these discriminators are actually valid.\n",
    "ikke - ikkje\n",
    "jeg - eg\n",
    "fra - frå\n",
    "sier - seier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
